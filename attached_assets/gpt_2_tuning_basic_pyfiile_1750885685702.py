# -*- coding: utf-8 -*-
"""Copy of GPT-2 tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qL-QgzzUic35T78x-oY4VlTDpYIB_WdH
"""

# Install dependencies
!pip install -U datasets huggingface_hub fsspec transformers

# Load dataset
from datasets import load_dataset
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1') #This is the data it is tuning on, this can be academic papers, business magazines etc

# Clean dataset
def clean_text(example):
    example['text'] = example['text'].strip().replace('\n', ' ')
    return example

dataset = dataset.map(clean_text)

# Load pretrained GPT-2 tokenizer and model
from transformers import AutoTokenizer, GPT2LMHeadModel

tokenizer = AutoTokenizer.from_pretrained("gpt2")

# GPT-2 has no pad token by default
tokenizer.pad_token = tokenizer.eos_token

model = GPT2LMHeadModel.from_pretrained("gpt2")

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Setup data collator
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # For causal language modeling
)

# Define training arguments
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./gpt2_finetuned_output",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=500,
    logging_steps=100,
    report_to="none"  # Disable wandb
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"].select(range(10000)),
    tokenizer=tokenizer
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("gpt2_finetuned")
tokenizer.save_pretrained("gpt2_finetuned")